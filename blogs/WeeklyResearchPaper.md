### Weekly research paper review (in short)
The key motivation is to learn from a research paper every week (whatever I can) and maintain a log of the same.
#### October <br>
**Week 1:** Learning transferable visual models from natural language supervision
- **Authors** : A. Radford, J. W. Kim, et al
- **Journal** : ICML 2021
- **Link** : https://arxiv.org/abs/2103.00020
- **In 1 sentence** <br> Zero shot Image caption generated using image and text encoding.
- **In 10 sentences** <br> Novel approach to image captioning using encodings for image and text related to image. It is inspired by two main models: (a) VirTex: shows image captioning models are really good for tasks like image classification (something like transfer learning)(b) ConVirt : it shows independently training words and images to improve model with less data. The main idea is to maximize the cosine similarity of image and text pairs. Since _WIT_ is large (low risk of overfitting) both encoders are trained from scratch and linear projections (rather than non-linear) used between the representations and the shared embedding space, since no difference was observed during training. ViT-L/14-336px (Trained with FixRes (Touvron et al., 2019)) and a 12 head transformer yielded the best accuracy for the CLIP model.
Zero-shot transfer can be thought of as assessing the task learning ability of a model: A dataset evaluates performance on a task on a specific distribution, the zero-shot transfer focus is inspired by works illustrating task learning in NLP. Evaluate visual representation quality via linear probes: Linear (rather than non-linear) probes are used to avoid the introduction of additional hyperparameter and cost. In zero-shot transfer, using text class labels can present challenges:
Some datasets only provide integer class id labels (these cannot be used). One issue is _polysemy_ - the word sense is ambiguous without context these are resolved by two methods: (i) Prompt Templates and (ii) Customized templates. Prompt Ensembling: Ensembling is performed over the embeddings, rather than predicted probabilities to enable caching so that the cost is amortised over predictions helping to boost the performance by 3.5% .

- **Datasets used** : aYahoo, ImageNet, SUN.
<br>

**Week 2: On Compositions of Transformations in Contrastive Self-Supervised Learning** 
- **Authors** : Mandela Patrick, Yuki M. Asano, et al.
- **Journal** : ICCV 2020
- **Link** : https://arxiv.org/pdf/2003.04298.pdf
- **In 1 sentence** <br> How invariance and distinctiveness to individual transformation can be used to learn meaningfully from video.
- **In 10 sentences** <br> 
Basic idea is to generalize contrastive methods such as CPC, PIRL and etc to learn representations that can be invariant or distinctive to any number of transformations.GDT are Generalized Data Transformation like data augmentation (like cropping or rotating) or extracting a specific image or video from the collection or extracting a specific modality from a video. We wish to generalize this construction to richer data such as videos. Compared to images, videos contain multi- ple modalities and additional dimensions, which allows to consider qualitatively different transformations such as time shift, time reversal, and modality slicing. This generalization is however non-trivial. We leverage the noise contrastive loss (Noise contrastive losses measure the similarity between sample pairs in a representational space and are at the core of several recent works on unsupervised feature learning) as a learning framework to encourage the network to learn desired invariance and distinctiveness to data transformations. We show that GDT’s batch sampling strategy is statistically more efficient than naively-sampled pairs for contrastive learning to do this by showing that GDT’s objective has the same mean but a lower variance than sampling batches, which would either enumerate all possible pairs of transformations (which is prohibitively expensive) or subsample it by sampling transformations independently.
**Datasets:** Kinetics-400,  HT100M <br>
**Week 3: Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval** 
- **Authors** : Max Bain, Arsha Nagrani et. al.
- **Journal** : ICCV 2021
- **Link** : https://openaccess.thecvf.com/content/ICCV2021/papers/Bain_Frozen_in_Time_A_Joint_Video_and_Image_Encoder_for_ICCV_2021_paper.pdf
- **In 1 sentence** <br> Space-time transformer encoder allows us to train flexibly on both images and videos with captions jointly, by treating an image as a single frame video.
- **In 10 sentences** <br> The rapid development of joint visual-text models is due to the usual improvements on three fronts: new neural network architectures (e.g. transformers for both text and visual inputs); new large-scale datasets; and new loss functions that are, for example, able to handle label noise However the only work either for images or videos. The dual encoder architecture treats images as videos that are 'frozen in time'.Us- ing a transformer-based architecture allows us to train with variable-length sequences, treating an image as if it was a single frame video, unlike in standard 3D CNNs where to train on images jointly with videos one must incur the cost of actually generating a static video. Main contributions: (i) This model does not require pre-training and is trained end-to-end on inspired by employs a transformer architecture with a modified divided space- time attention applied directly to pixels; (ii) ---? (iii) New video text dataset.
The visual encoder takes as input an image or video clip X ∈ R^M×3×H×W^ consisting of M frames of resolution H × W , where M = 1 for images. 
Following the protocol in ViT and Timesformer, the input video clip is divided into M ×N non-overlapping spatiotemporal patches of size P × P, where N = HW/P^2^. The patches x ∈ R^M×N×3×P×P^ are fed through a 2D convolutional layer and the output is flattened, forming a sequence of embeddings z ∈ RM N ×D for input to the transformer, where D depends of the number of kernels in the convolutional layer. The video sequence is fed into a stack of space-time transformer blocks, by replacing the residual connection between the block input and the temporal attention output with a residual connection between the block input and the spatial attention output there is great improvement in results attained. ach block sequentially performs temporal self- attention and then spatial self-attention on the output of previous block. The video clip embedding is obtained from the [CLS] token of the final block. Both text and video encodings are projected to a common dimension via single linear layers. We compute the similarity between text and video by performing the dot product between the two projected embeddings. Loss: We employ in a retrieval setting, where matching text-video pairs in the batch are treated as positives, and all other pairwise combinations in the batch are treated as negatives. Joint image-video training. In this work, we train jointly on both image-text pairs as well as video-text pairs, tak- ing advantage of both for larger-scale pre-training. Our joint training strategy involves alternating batches between the image and video datasets.
**Datasets:** WebVid-2M {introduced} <br>